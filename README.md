# Hotel Review Score Prediction Using Deep Learning

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![TensorFlow](https://img.shields.io/badge/TensorFlow-2.8+-orange.svg)
![License](https://img.shields.io/badge/License-Academic-green.svg)
![Status](https://img.shields.io/badge/Status-Complete-success.svg)
![DL](https://img.shields.io/badge/DL-Regression-red.svg)

A regression project predicting hotel review scores using a hybrid BiLSTM-MLP architecture that combines textual reviews with structured hotel and reviewer metadata. Developed as part of the Machine Learning, Artificial Neural Networks and Deep Learning course (June 2025 exam session) for the Bachelor in Artificial Intelligence at the University of Milan (UNIMI).

<p align="center">
  <img src="images/architecture_diagram.png" alt="Model Architecture" width="800"/>
</p>

## ğŸ¯ Project Overview

Hotel review analysis is crucial for the hospitality industry to understand customer satisfaction and improve service quality. This project employs deep learning techniques to predict numerical review scores from both textual content and structured features, enabling automated quality assessment.

**Objective:** Build a robust regression model capable of predicting hotel review scores on a 0-10 scale using multi-modal input data (text + structured features).

### ğŸ† Key Achievements

- **Test MSE:** 0.0331 (normalized scale) â‰ˆ **1.82 point error** on 0-10 scale
- **Hybrid architecture** combining BiLSTM for text and MLP for structured features
- **Systematic hyperparameter tuning** with K-Fold cross-validation
- **Comprehensive preprocessing** pipeline for text and multi-type features

## ğŸ“Š Dataset

**Source:** Course-provided dataset via [University of Milan](http://frasca.di.unimi.it/MLDNN/input_data.pkl)

The dataset contains hotel reviews from 13,772 visitors with comprehensive metadata:

### Features
- **Textual:** Review text (up to 400 words)
- **Hotel Information:** Hotel Name, Hotel Address, Total Reviews for Hotel
- **Reviewer Information:** Reviewer Nationality, Total Reviews by Reviewer
- **Temporal:** Review Date (month/day/year format)
- **Ratings:** Review Score (2.5-10 range, float), Review Type (Good/Bad)

### Target Variable
- **Review_Score:** Continuous regression (float in [0, 10] range)

### Dataset Statistics
- **Total Samples:** 13,772 hotel reviews
- **Features Used:** 6 (after preprocessing)
- **Unique Hotels:** 1,298
- **Vocabulary Size:** 9,639 unique words
- **Score Range:** 2.5 - 10.0

## ğŸ”§ Methodology

### 1. Data Preprocessing

**Feature Engineering:**
- **Retained features:** `Hotel_Name`, `Review_Date`, `Hotel_number_reviews`, `Reviewer_number_reviews`, `Review`, `Review_Score`
- **Dropped features:** `Hotel_Address`, `Reviewer_Nationality`, `Review_Type`, `Average_Score`
- **Date decomposition:** Split `Review_Date` into `Day`, `Month`, `Year` as separate integer features
- **Target normalization:** Scaled `Review_Score` from [0,10] to [0,1] for sigmoid output

**Rationale for Feature Selection:**
- `Hotel_Address` and `Reviewer_Nationality` considered irrelevant for prediction
- `Review_Type` excluded to avoid target leakage (directly correlates with score)
- `Average_Score` not part of original assignment features

**Missing Data Handling:**
- Dataset contained no missing values after feature selection
- All 13,772 samples retained for modeling

### 2. Feature Transformation Pipeline

Created specialized preprocessing for different feature types:

| Feature Type | Transformation | Applied To |
|--------------|----------------|------------|
| Text | Tokenization, Lowercasing, Punctuation Removal, Padding | Review |
| Categorical (High-cardinality) | One-Hot Encoding | Hotel_Name |
| Numerical | MinMax Scaling [0,1] | Reviewer_number_reviews, Hotel_number_reviews |
| Temporal | Split + MinMax Scaling | Day, Month, Year |

**Text Preprocessing Details:**
```python
1. Tokenization by whitespace
2. Lowercase conversion
3. Punctuation removal
4. Non-alphabetic token filtering
5. Vocabulary construction (9,639 words)
6. Sequence padding to length 100
7. Unknown word handling with <UNK> token (index 0)
```

### 3. Model Selection & Optimization

**Approach:** Manual Randomized Search with K-Fold Cross-Validation

**Architecture Selection Rationale:**

| Architecture | Pros | Cons | Decision |
|--------------|------|------|----------|
| **BiLSTM + MLP** â­ | Sequential text understanding, bidirectional context, fuses multi-modal data | More parameters, slower training | **Selected** - Optimal for text+structured data |
| Unidirectional LSTM | Fewer parameters, faster | Only forward context | Inferior - Limited context |
| CNN | Fast, parallel processing | Local patterns only, weak on long dependencies | Not suitable - Reviews need full context |
| MLP only | Simplest, fastest | No sequential understanding | Not suitable - Ignores word order |

**Hyperparameter Search:**
- **Search space:** 24 total combinations
- **Sampled:** 5 random configurations
- **CV strategy:** 2-fold for efficiency
- **Evaluation metric:** Mean Squared Error (MSE)
- **Primary metric:** F1 score â†’ MSE (regression task)

**Why Not GridSearchCV?**
- KerasRegressor incompatibility with multi-input models (text + structured)
- Scikit-learn wrappers don't support Keras functional API with multiple inputs
- Manual implementation provides better control and flexibility

### 4. Final Model Architecture

```python
Model: BiLSTM-MLP Hybrid
â”œâ”€â”€ Text Input Branch (100,)
â”‚   â”œâ”€â”€ Embedding Layer (vocab_size=9640, dim=150)
â”‚   â”œâ”€â”€ Bidirectional LSTM (64 units Ã— 2 = 128 output)
â”‚   â””â”€â”€ Output: (None, 128)
â”‚
â”œâ”€â”€ Structured Input Branch (1303,)
â”‚   â”œâ”€â”€ Hotel_Name (One-Hot): 1298 features
â”‚   â”œâ”€â”€ Numerical Features (Scaled): 5 features
â”‚   â””â”€â”€ Output: (None, 1303)
â”‚
â”œâ”€â”€ Fusion Layer
â”‚   â”œâ”€â”€ Concatenate: [BiLSTM, Structured] â†’ (None, 1431)
â”‚   â”œâ”€â”€ Dense(64, activation='sigmoid')
â”‚   â”œâ”€â”€ Dropout(0.2)
â”‚   â”œâ”€â”€ Batch Normalization
â”‚   â””â”€â”€ Dense(1, activation='sigmoid') â†’ [0,1]
â”‚
â””â”€â”€ Output: Rescaled to [0,10]
```

**Optimal Hyperparameters:**
- Embedding dimension: 150
- LSTM units: 16 (per direction)
- Dropout rate: 0.2
- Learning rate: 0.0001
- Batch size: 64
- Optimizer: Adam
- Loss function: Mean Squared Error (MSE)

## ğŸ“ˆ Results

### Model Performance

| Metric | Test Set Score | Description |
|--------|----------------|-------------|
| **Test MSE (normalized)** | **0.0331** | Mean squared error on [0,1] scale |
| **Test MSE (original)** | **~3.31** | Mean squared error on [0,10] scale |
| **RMSE** | **~1.82** | Root mean squared error (average error in points) |
| **Total Parameters** | 632,849 | Model complexity (2.41 MB) |
| **Training Time** | ~20-25s/epoch | On GPU (Google Colab) |

### Performance Insights

**Learning Behavior:**
- Epoch 1: Training MSE ~0.15, Validation MSE ~0.05
- Epoch 2: Training MSE ~0.12, Validation MSE ~0.03
- Consistent improvement across epochs
- Low gap between train and validation indicates good generalization

**Prediction Examples:**
| Actual Score | Predicted Score | Error |
|--------------|-----------------|-------|
| 7.1 | 7.8 | +0.7 |
| 6.3 | 8.3 | +2.0 |
| 5.8 | 8.4 | +2.6 |
| 4.2 | 4.7 | +0.5 |
| 2.5 | 4.1 | +1.6 |

**Key Observations:**
- Model performs well on extreme scores (very low/high)
- Some tendency to predict slightly higher than actual (positive bias)
- Errors typically within Â±2 points
- Most predictions fall within acceptable range for practical use

## ğŸš€ Installation & Usage

### Prerequisites
```bash
Python 3.8 or higher
TensorFlow 2.8+
```

### Quick Start

1. **Clone the repository:**
```bash
git clone https://github.com/yourusername/hotel-review-prediction.git
cd hotel-review-prediction
```

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Download the dataset:**
```bash
wget http://frasca.di.unimi.it/MLDNN/input_data.pkl
```

4. **Run the notebook:**
```bash
jupyter notebook DiPilato_535298.ipynb
```

### Alternative: Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/yourusername/hotel-review-prediction/blob/main/DiPilato_535298.ipynb)

The notebook automatically downloads the dataset in the first cell.

## ğŸ“ Project Structure

```
hotel-review-prediction/
â”‚
â”œâ”€â”€ DiPilato_535298.ipynb         # Main implementation notebook
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ README.md                      # Project documentation (this file)
â”œâ”€â”€ ARCHITECTURE.md                # Detailed technical documentation
â”œâ”€â”€ LICENSE                        # MIT License
â”œâ”€â”€ .gitignore                     # Git ignore rules
â”‚
â”œâ”€â”€ docs/                          # Supporting documents
â”‚   â”œâ”€â”€ exam_test.pdf              # Original exam assignment
â”‚   â””â”€â”€ EXAM_DL.pdf                # Written exam answers
â”‚
â””â”€â”€ images/                        # Visualizations (optional)
    â”œâ”€â”€ architecture_diagram.png
    â”œâ”€â”€ training_history.png
    â””â”€â”€ predictions_scatter.png
```

## ğŸ” Key Features

âœ… **Multi-modal learning** combining text and structured features  
âœ… **Bidirectional LSTM** for comprehensive text understanding  
âœ… **Custom preprocessing pipeline** for diverse feature types  
âœ… **Robust vocabulary construction** with unknown token handling  
âœ… **Systematic hyperparameter optimization** via randomized search  
âœ… **Dual-input architecture** using Keras functional API  
âœ… **Reproducible results** with fixed random seeds (seed=42)

## ğŸ§ª Technical Highlights

### Preprocessing Innovation
- **Vocabulary construction:** Built from training data only (9,639 unique words)
- **Unknown token handling:** Reserved index 0 for `<UNK>` to handle unseen words gracefully
- **Sequence padding:** Fixed length of 100 tokens with post-padding strategy
- **One-hot encoding:** Handled 1,298 unique hotels without dimensionality explosion (via sparse representation)

### Architecture Design Decisions

**Why BiLSTM?**
1. âœ… Captures sequential dependencies in reviews
2. âœ… Bidirectional processing provides richer context
3. âœ… Proven effectiveness for sentiment and rating prediction
4. âœ… Better than CNN for long-range dependencies in text

**Why Sigmoid in Hidden Layer?**
- Originally proposed in written exam for consistency
- Alternative: ReLU (faster, avoids vanishing gradients) - potential future improvement

**Why Single MLP Layer?**
- BiLSTM handles most feature learning
- MLP primarily fuses representations
- Regularization (dropout + batch norm) prevents overfitting

### Validation Strategy
- **Data split:** 70% train / 15% validation / 15% test
- **K-Fold CV:** 2-fold during hyperparameter search
- **Stratification:** Not applicable (regression task)
- **Multiple metrics:** MSE, RMSE for comprehensive evaluation

## ğŸ“ Academic Context

This project demonstrates proficiency in:
- Multi-modal deep learning architecture design
- Text preprocessing and embedding techniques
- Bidirectional RNN (BiLSTM) implementation
- Hybrid model construction with Keras functional API
- Handling mixed data types (text + categorical + numerical)
- Hyperparameter optimization strategies
- Model evaluation for regression tasks

**Course:** 509486 - Machine Learning, Artificial Neural Networks and Deep Learning  
**Exam Session:** June 19, 2025  
**Academic Year:** 2024/2025  
**Institution:** University of Milan (UNIMI)  
**Degree Program:** [L-31] Bachelor in Artificial Intelligence  
**Student ID:** 535298

## ğŸ“ Implementation Notes

### Changes from Written Proposal

The implementation includes several improvements over the original written exam answers:

| Written Proposal | Implementation | Rationale |
|------------------|----------------|-----------|
| Label Encoding for Hotel_Name | **One-Hot Encoding** | Avoids artificial ordinal relationships between hotels |
| No unknown token handling | **`<UNK>` token at index 0** | Robust handling of words not seen during training |
| Implicit sequence handling | **Explicit padding to length 100** | Required for uniform input shape to neural network |
| GridSearchCV | **Manual randomized search with K-Fold** | KerasRegressor incompatibility with multi-input models |
| Basic tokenization | **+ Non-alphabetic filtering** | Reduces vocabulary noise by removing numbers, dates |

All changes are thoroughly documented in the notebook with clear explanations.

## ğŸ”® Future Improvements

### Short-term
- [ ] Implement attention mechanism for interpretable word importance
- [ ] Add early stopping and learning rate scheduling
- [ ] Experiment with pre-trained embeddings (Word2Vec, GloVe)
- [ ] Visualize embedding space with t-SNE/UMAP

### Medium-term
- [ ] Try transformer-based models (BERT, RoBERTa)
- [ ] Explore ensemble methods (multiple BiLSTM models)
- [ ] Add review sentiment as auxiliary task (multi-task learning)
- [ ] Implement model explainability (LIME, SHAP)

### Long-term
- [ ] Develop web application for real-time prediction (Streamlit/Flask)
- [ ] Multi-language support with multilingual embeddings
- [ ] Aspect-based sentiment analysis (room, service, location scores)
- [ ] Integrate with hotel booking platforms

## ğŸ“š References

1. Dataset: Course materials - University of Milan, Department of Computer Science
2. [Keras Documentation](https://keras.io/) - Neural network implementation
3. [TensorFlow Guide](https://www.tensorflow.org/) - Deep learning framework
4. [scikit-learn](https://scikit-learn.org/) - Preprocessing utilities
5. Hochreiter & Schmidhuber (1997) - Long Short-Term Memory networks
6. Schuster & Paliwal (1997) - Bidirectional Recurrent Neural Networks

## ğŸ‘¨â€ğŸ’» Author

**Matteo Di Pilato**  
Bachelor in Artificial Intelligence  
University of Milan (UNIMI)  
Student ID: 535298  
Academic Year 2024/2025

ğŸ“§ Contact: [Your email if you want to add it]  
ğŸ”— GitHub: [pdmdp](https://github.com/pdmdp)  
ğŸ”— LinkedIn: [Your LinkedIn if you want to add it]

## ğŸ“„ License

This project is available under the MIT License. See [LICENSE](LICENSE) file for details.

**Academic Use:** This project was developed for academic purposes as part of the Machine Learning, Artificial Neural Networks and Deep Learning course exam at the University of Milan.

## ğŸ™ Acknowledgments

- **Dataset:** University of Milan, Department of Computer Science
- **Course Instructors:** ML, ANN, and Deep Learning teaching team
- **Institution:** University of Milan (UNIMI)
- **Libraries:** TensorFlow/Keras, scikit-learn, NumPy, Pandas communities
- **Inspiration:** Previous ML projects and course materials

---

<p align="center">
  â­ If you found this project helpful, please consider giving it a star!<br>
  ğŸ’¡ Feel free to fork and adapt for your own learning<br>
  ğŸ“š Check out my other ML projects: <a href="https://github.com/pdmdp/student-depression-project">Student Depression Prediction</a>
</p>

---

**Note:** This is an educational project. The model demonstrates deep learning concepts and should not be used for commercial hotel rating systems without further validation and ethical considerations.
